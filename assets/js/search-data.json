{
  
    
        "post0": {
            "title": "Optimizing SQL and Python Pipelines for Data Science",
            "content": "Review SQL Best Practices . Why the FROM statement is so important . The FROM statement determines how you will structure the rest of your query | You want your FROM table to be a core table with the fewest columns &amp; rows that is highly indexed | . SELECT users.* SELECT users.* FROM users usr FROM orders or INNER JOIN orders or INNER JOIN users usr ON usr.customer_id = or.customer_id ON usr.customer_id = or.customer_id WHERE or.order_date &gt; WHERE or.order_date &gt; date_sub(current_date, INTERVAL 30 days) date_sub(current_date, INTERVAL 30 days) . 13 seconds to run - 19 seconds to run . Two queries that return exact same results. The only difference is we flip the tables, and it makes 6 seconds speed-up. . Why Temporary Tables Rock . Temporary tables improve readability over (nested) subqueries, i.e. | . SELECT ... INTO #goodnameforatemptable SELECT ... FROM blah JOIN #goodnameforatemptable instead of SELECT ... FROM JOIN (SELECT ... ) ON . Keeps your code readable and makes troubleshooting much easier. | Helps you follow the single responsibility principle. | The Query Optimizer may not be able to properly optimize a query with subqueries and will likely result in longer run times. ### Some Simple SQL Optimization things to remember . | When doing wildcard searches use something% (limit it to backend wildcard search) vs. %something% if you can. . | Functions on indexed columns in the where clause remove the indexing. | . WHERE substring (column, 1, 1) = &#39;F&#39; . vs. . WHERE column LIKE &#39;F%&#39; . Don&#39;t pull in columns you don&#39;t need. | Move filters from the WHERE statement to the JOIN condition if using an OUTER JOIN. | Use your indices as much as possible. | If you can, use UNION ALL instead of UNION DISTINCT. | . Data Manipulation &amp; Feature Engineering Before Python . Joining Multiple Data Sources . This is SQL&#39;s bread and butters, it&#39;s wheelhouse, what it was made to do. | Even if you have multiple raw data files, I advocate for standing up a quick DB and loading the CSVs in there vs. bringing them straight into pandas &mdash; you don&#39;t have to reload the file every time you want to look at the data. | . Narrowing down your dataset . More data isn&#39;t always better &mdash; loading 50 million rows of data straight into memory will be incredibly time consuming, inefficient, and does not necessarily lead to better machine learning scores. | Time frame considerations &mdash; do you see a dramatic improvement in model scores with two years of data vs. one year? | . Learning Curves to Help Determine Dataset Sizes . Learning Curves help you understand not only your algorithm&#39;s bias vs. variance but also how many records you need to train a model you&#39;re happy with. . High variance &mdash; If a learning algorithm is suffering from high variance, getting more training data is likely to help. . | High bias &mdash; If a learning algorithm is suffering from high bias, getting more training data will not (by itself) help much. . | . Feature Engineering . Case statements prevent a size mismatch that can sometimes happen with pandas.get_dummies in train vs test sets. | Lead, Lag, Rank functions are powerful calculations that are oddly written in SQL but can leverage indices and run more optimally in SQL. | A lot of features can be created before you even load the data if you&#39;d like. | . Pandas . You have the whole Python toolset at your disposal. | You can do more advanced and intensive data manipulation in code that is easily readable and testable. | Easy integration with data visualization libraries, jupyter notebooks, and functions like data_frame.describe() make it ideal for EDA (exploratory data analysis). | . How to Optimize Reading and Writing with Python . Reading in data from SQL . pandas.read_sql() and pandas.read_gbq() are notoriously slow. | The best way I have found to do this is to save your final query as a table and export that table to CSV or CSV.GZ and then load the CSV into pandas via pandas.read_csv() | What this also allows for is then each time moving forward you only have to load the CSV directly. | . Writing data to SQL . Similarly, data_frame.to_sql(), data_frame.to_gbq(), even Spark&#39;s data_frame.write.jdbc() are also slow. . Batch writing to the rescue. . | SQLAlchemy &amp; Pandas:pythonSession = sessionmaker(bind=dest_db_con) sess = Session() sess.bulk_insert_mappings(MentorInformation, df.to_dict(orient=&quot;records&quot;)) sess.close() . | Spark: . | . jdbcUrl = &#39;jdbc:mysql://{}:3306/{}?useServerPrepStmts=false&amp;rewriteBatchedStatements=true&amp;user{}&amp;password={}&#39; . &amp;rewriteBatchedStatements=true . With these steps, I&#39;ve seen jobs that used to take hours now take minutes. . Conclusion: . Knowing what tool is right for the job is extremely powerful. | . Notes from: . https://www.youtube.com/watch?v=H5FNFxHgSj8&amp;list=LLqaZUbmWbk33CkdqqWfC1xw&amp;index=2 | .",
            "url": "https://luiggi629.github.io/wojtek-zubera/optpysql/",
            "relUrl": "/optpysql/",
            "date": " • Sep 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Semantic Commit Messages",
            "content": "Format: &lt;type&gt;(&lt;scope&gt;): &lt;subject&gt; . &lt;scope&gt; is optional . Example . feat: add hat wobble ^--^ ^^ | | | +-&gt; Summary in present tense. | +-&gt; Type: chore, docs, feat, fix, refactor, style, or test. . More Examples: . feat: (new feature for the user, not a new feature for build script) | fix: (bug fix for the user, not a fix to a build script) | docs: (changes to the documentation) | style: (formatting, missing semi colons, etc; no production code change) | refactor: (refactoring production code, eg. renaming a variable) | test: (adding missing tests, refactoring tests; no production code change) | chore: (updating grunt tasks etc; no production code change) | . References: . https://gist.github.com/joshbuchea/6f47e86d2510bce28f8e7f42ae84c716 | https://www.conventionalcommits.org/ | https://seesparkbox.com/foundry/semantic_commit_messages | http://karma-runner.github.io/1.0/dev/git-commit-msg.html | .",
            "url": "https://luiggi629.github.io/wojtek-zubera/semantic/",
            "relUrl": "/semantic/",
            "date": " • Sep 18, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Git Tips",
            "content": "When working with multiple branches you can stash changes away that you are not ready to commit yet: . git stash save &quot;save message&quot; . Then you can retrieve them later with: . git stash list git stash pop &lt;stash_item_hash&gt; . use: . git stash apply . instead of: . git stash pop &lt;stash_item_hash&gt; . when you think you&#39;ll want to use those stashed changes again later . When you want to add more changes to last (local) commit, use: . git add file(s) git commit --amend . It opens your editor and lets you modify the last commit. . When I want to amend on last commit, usually I don&#39;t want to change my message, so: . git commit —amend —no-edit . is a time savers! . On the other hand, if you want to spread out your changes over multiple commits (granular commits are good!), use: . git add -p . Here you can commit only parts (&quot;hunks&quot;) of your changes in interactive mode. . &quot;-p&quot; is nice for git add, but helps with other commands too (e.g. reset, checkout, log) . Imagine you got all excited coding, but you committed them on the main branch. . No problem: if you forgot to branch off earlier, you can still do so at this point: . git branch new_branch . Now you have your changes on both original and new branch so you can wipe them out on former: . git checkout original_branch git reset HEAD~&lt;number-of-commits-to-go-back&gt; . (Make sure this is all local though, don&#39;t go modify changes you already pushed!) . If you want to use a commit from one branch on another, you can cherry pick it: . git cherry-pick &lt;commit_from_anywhere_in_your_repo&gt; . If you want to change a commit that has already been pushed (for example if you want to make some changes after a PR review), you can use fixup commits and rebase with autosquash: . stage changes | git commit --fixup &lt;commit hash to update&gt; git rebase -i --autosquash &lt;branch to rebase onto&gt; git push --force-with-lease . References: . https://www.linkedin.com/in/bbelderbos | .",
            "url": "https://luiggi629.github.io/wojtek-zubera/gitips/",
            "relUrl": "/gitips/",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Properly Fork and Generate Pull Requests",
            "content": "Copyright . Copyright 2017, Chase Pettit, https://gist.github.com/Chaser324/ce0505fbed06b947d962 . MIT License, http://www.opensource.org/licenses/mit-license.php . Whether you&#39;re trying to give back to the open source community or collaborating on your own projects, knowing how to properly fork and generate pull requests is essential. Unfortunately, it&#39;s quite easy to make mistakes or not know what you should do when you&#39;re initially learning the process. I know that I certainly had considerable initial trouble with it, and I found a lot of the information on GitHub and around the internet to be rather piecemeal and incomplete - part of the process described here, another there, common hangups in a different place, and so on. . In an attempt to coallate this information for myself and others, this short tutorial is what I&#39;ve found to be fairly standard procedure for creating a fork, doing your work, issuing a pull request, and merging that pull request back into the original project. . Creating a Fork . Just head over to the GitHub page and click the &quot;Fork&quot; button. It&#39;s just that simple. Once you&#39;ve done that, you can use your favorite git client to clone your repo or just head straight to the command line: . # Clone your fork to your local machine git clone git@github.com:USERNAME/FORKED-PROJECT.git . Keeping Your Fork Up to Date . While this isn&#39;t an absolutely necessary step, if you plan on doing anything more than just a tiny quick fix, you&#39;ll want to make sure you keep your fork up to date by tracking the original &quot;upstream&quot; repo that you forked. To do this, you&#39;ll need to add a remote: . # Add &#39;upstream&#39; repo to list of remotes git remote add upstream https://github.com/UPSTREAM-USER/ORIGINAL-PROJECT.git # Verify the new remote named &#39;upstream&#39; git remote -v . Whenever you want to update your fork with the latest upstream changes, you&#39;ll need to first fetch the upstream repo&#39;s branches and latest commits to bring them into your repository: . # Fetch from upstream remote git fetch upstream # View all branches, including those from upstream git branch -va . Now, checkout your own master branch and merge the upstream repo&#39;s master branch: . # Checkout your master branch and merge upstream git checkout master git merge upstream/master . If there are no unique commits on the local master branch, git will simply perform a fast-forward. However, if you have been making changes on master (in the vast majority of cases you probably shouldn&#39;t be - see the next section, you may have to deal with conflicts. When doing so, be careful to respect the changes made upstream. . Now, your local master branch is up-to-date with everything modified upstream. . Doing Your Work . Create a Branch . Whenever you begin work on a new feature or bugfix, it&#39;s important that you create a new branch. Not only is it proper git workflow, but it also keeps your changes organized and separated from the master branch so that you can easily submit and manage multiple pull requests for every task you complete. . To create a new branch and start working on it: . # Checkout the master branch - you want your new branch to come from master git checkout master # Create a new branch named newfeature (give your branch its own simple informative name) git branch newfeature # Switch to your new branch git checkout newfeature . Now, go to town hacking away and making whatever changes you want to. . Submitting a Pull Request . Cleaning Up Your Work . Prior to submitting your pull request, you might want to do a few things to clean up your branch and make it as simple as possible for the original repo&#39;s maintainer to test, accept, and merge your work. . If any commits have been made to the upstream master branch, you should rebase your development branch so that merging it will be a simple fast-forward that won&#39;t require any conflict resolution work. . # Fetch upstream master and merge with your repo&#39;s master branch git fetch upstream git checkout master git merge upstream/master # If there were any new commits, rebase your development branch git checkout newfeature git rebase master . Now, it may be desirable to squash some of your smaller commits down into a small number of larger more cohesive commits. You can do this with an interactive rebase: . # Rebase all commits on your development branch git checkout git rebase -i master . This will open up a text editor where you can specify which commits to squash. . Submitting . Once you&#39;ve committed and pushed all of your changes to GitHub, go to the page for your fork on GitHub, select your development branch, and click the pull request button. If you need to make any adjustments to your pull request, just push the updates to GitHub. Your pull request will automatically track the changes on your development branch and update. . Accepting and Merging a Pull Request . Take note that unlike the previous sections which were written from the perspective of someone that created a fork and generated a pull request, this section is written from the perspective of the original repository owner who is handling an incoming pull request. Thus, where the &quot;forker&quot; was referring to the original repository as upstream, we&#39;re now looking at it as the owner of that original repository and the standard origin remote. . Checking Out and Testing Pull Requests . Open up the .git/config file and add a new line under [remote &quot;origin&quot;]: . fetch = +refs/pull/*/head:refs/pull/origin/* . Now you can fetch and checkout any pull request so that you can test them: . # Fetch all pull request branches git fetch origin # Checkout out a given pull request branch based on its number git checkout -b 999 pull/origin/999 . Keep in mind that these branches will be read only and you won&#39;t be able to push any changes. . Automatically Merging a Pull Request . In cases where the merge would be a simple fast-forward, you can automatically do the merge by just clicking the button on the pull request page on GitHub. . Manually Merging a Pull Request . To do the merge manually, you&#39;ll need to checkout the target branch in the source repo, pull directly from the fork, and then merge and push. . # Checkout the branch you&#39;re merging to in the target repo git checkout master # Pull the development branch from the fork repo where the pull request development was done. git pull https://github.com/forkuser/forkedrepo.git newfeature # Merge the development branch git merge newfeature # Push master with the new feature merged into it git push origin master . Now that you&#39;re done with the development branch, you&#39;re free to delete it. . git branch -d newfeature . Additional Reading . Atlassian - Merging vs. Rebasing | . Sources . GitHub - Fork a Repo | GitHub - Syncing a Fork | GitHub - Checking Out a Pull Request | .",
            "url": "https://luiggi629.github.io/wojtek-zubera/github-forking/",
            "relUrl": "/github-forking/",
            "date": " • Sep 9, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Python Environment for Data Science",
            "content": "Python Environment . Interpreter . If you have programs that require different python versions or programs that depend on different versions of the same third-party module and you want to switch between those programs seamlessly? . Pyenv will help you doing that! . You can install Pyenv by . curl https://pyenv.run | bash . After that, add the following lines to your .bashrc (same for .zshrc) to have pyenv available in your terminal . export PATH=&quot;~/.pyenv/bin:$PATH&quot; eval &quot;$(pyenv init -)&quot; eval &quot;$(pyenv virtualenv-init -)&quot; . I had to restart my terminal afterwards . On Ubuntu install the following ones to not run into problems . sudo apt-get install build-essential libsqlite3-dev sqlite3 bzip2 libbz2-dev zlib1g-dev libssl-dev openssl libgdbm-dev libgdbm-compat-dev liblzma-dev libreadline-dev libncursesw5-dev libffi-dev uuid-dev . Now, to install a python interpreter just do . pyenv install VERSION_YOU_WOULD_LIKE_TO_INSTALL . You can list out all versions available via pyenv . pyenv install --list . To make it concrete, let’s install python 3.8.2 and make it your default global-interpreter . pyenv install 3.8.2 . pyenv global 3.8.2 . Dependency Management via poetry . The way the authors recommend installing poetry is . curl -sSL https://raw.githubusercontent.com/sdispater/poetry/master/get-poetry.py | python . Another way is using pip and pyenv-virtualenv. . Create a virtual environment called tools that is based on 3.8.2 . pyenv virtualenv 3.8.2 tools . Install poetry into the tools virtual env . pyenv activate tools . python -m pip install poetry . Check installed poetry version . poetry --version . Leave the virtual env . pyenv deactivate . Add your tools virtual env to the globally available ones . pyenv global 3.8.2 tools . I had to restart my terminal afterwards . You can start using poetry . poetry --version . Before using poetry I recommend configuring it, such that it creates your project’s virtual environment in a .venv folder inside the project directory. . poetry config virtualenvs.in-project true . Initialze a new project . poetry new ml-project cd ml-project . Add modules and create virtual environment. . poetry add pandas --extras all . Consistent Formatting and Readability . We add black as a development dependency with --dev as we don&#39;t need it when it comes to production . poetry add --dev black . I’d rather maintain the recommended 79 character length. I just need to configure my pyproject.toml to line-length=79 and everything is all set. Here’s my .toml file for configuring black: . [tool.black] line-length = 79 include = &#39; .pyi?$&#39; exclude = &#39;&#39;&#39; /( .git | .hg | .mypy_cache | .tox | .venv | _build | buck-out | build | dist )/ &#39;&#39;&#39; . poetry add --dev flake8 . Insight of your project&#39;s dependencies . poetry show --tree poetry show --latest . In order for black to work nicely with flake8, we need to list down some error codes to ignore. tox.ini configuration below: . touch tox.ini . . # Flake8 Configuration [flake8] ignore = E203, D203, E266, E501, W503, F403, F401 exclude = .tox, .git, __pycache__, docs/source/conf.py, build, dist, tests/fixtures/*, *.pyc, *.egg-info, .cache, .eggs max-line-length = 79 max-complexity = 18 select = B,C,E,F,W,T4,B9 format = ${cyan}%(path)s${reset}:${yellow_bold}%(row)d${reset}:${green_bold}%(col)d${reset}: ${red_bold}%(code)s${reset} %(text)s . mypy Type-Correctness . Through type annotations, your code becomes better to understand, maintain, and less prone to errors. Why less prone to errors? Because you can statically check if the types of your variables and functions match the expected ones. . poetry add --dev mypy . Running mypy might create a lot of errors. You can configure it to only warn you about the things you are interested in. You do that by adding a mypy.ini file to your project and refer to the documentation for more details. . Isort . Isort is a Python utility / library to sort imports alphabetically, and automatically separated into sections and by type. . poetry add --dev isort . Pre-commit . Pre-commit is a tool that executes checks before you commit code to your repository. When those checks fail, your commit will be rejected. With that, your repository will never see not formatted code, or none type-checked one, or anything else depending on the checks you are going to include. . You can either install it directly into your project using poetry or install it on your local machine. I prefer the latter, as pre-commit is only used locally and not on a CI/CD server. In contrast, black and mypy should run on a CI/CD server, thus, it makes sense to add them to the project’s dev dependencies. Here is how one could install it making use of the already existing tool virtual environment. . Install pre-commit into the tools virtual env . pyenv activate tools . python -m pip install pre-commit . pyenv deactivate . pre-commit --version . To use it, you first need to add a config file called .pre-commit-config.yaml to the top-level folder of your project. In that file, you configure all the hooks that should run. . touch .pre-commit-config.yaml . . repos: - repo: https://github.com/psf/black rev: 20.8b1 hooks: - id: black language_version: python3.8 - repo: https://github.com/pre-commit/mirrors-mypy rev: v0.782 hooks: - id: mypy - repo: https://gitlab.com/pycqa/flake8 rev: 3.8.3 # pick a git hash / tag to point to hooks: - id: flake8 - repo: https://github.com/pycqa/isort rev: 5.5.1 hooks: - id: isort . In the top level folder run . pre-commit install . It is recommended to manually run pre-commit on all files as is it only touches the files that have been changed since the last commit. . pre-commit run --all --show-diff-on-failure . Now, the hooks will run on every commit. The black hook will not only check for formatting issues but also format the files accordingly. Whenever you add a new hook, so also at the very beginning, it is recommended to manually run pre-commit on all files as is it only touches the files that have been changed since the last commit . Results . So what we have is a pipeline that safeguards project against wrongly-formatted code &mdash; now we can focus on content. . Credits to: . Simon Hawe | LJ MIRANDA | .",
            "url": "https://luiggi629.github.io/wojtek-zubera/poetry/",
            "relUrl": "/poetry/",
            "date": " • Sep 8, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About me",
          "content": "Hi! I’m Wojtek, welcome to my page about hardest part of the job. I’m a former professional e-gamer, currently machine learning engineer. . When asked about the hardest part of the job, I reply it’s not machine learning, considering most our ML looks like: . from sklearn import Model Model.fit(features, target) predictions = model.predict(testing_features) . Instead, the hardest parts of data science are developing everything that occurs before and after modeling. Before we have: loading data from a database, feature engineering, data validation, and data processing pipelines (assuming our job starts after data is ingested). After we need to verify the results, set tasks to run automatically on a schedule, write results back to our database and send off web hooks to trigger other services. . I started this blog for two purposes: to write about what I’m learning, and to share my perspectives and reflections with others and my future self. The former reason function mainly to reinforce what I’m learning. Most of the blog posts are a living document (jupyter notebook), so I’ll come back and edit things. These are also informal posts, more of a quick brain dump than a work of art. To be honest, this is more of a journal than a blog, but I’ve chosen to make this public in the case of: . a) the content can help someone else . b) someone disagrees and we can have a discussion. . I appreciate feedback and constructive criticism. The best place to reach me is in the comments, or on LinkedIn .",
          "url": "https://luiggi629.github.io/wojtek-zubera/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://luiggi629.github.io/wojtek-zubera/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}