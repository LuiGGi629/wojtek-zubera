{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Optimizing SQL and Python Pipelines for Data Science\n",
    "> Poorly written SQL and Python can make data extraction and manipulation tedious and painful. Streamlined processes utilizing SQL best practices will save hours of frustration.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [python, pandas, sql, data-science]\n",
    "- permalink: /optpysql/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Review SQL Best Practices\n",
    "\n",
    "> Why the FROM statement is so important\n",
    "\n",
    "- The FROM statement determines how you will structure the rest of your query\n",
    "- You want your FROM table to be a core table with the fewest columns & rows that is highly indexed\n",
    "\n",
    "```sql\n",
    "SELECT users.*                              SELECT users.*\n",
    "FROM users usr                              FROM orders or\n",
    "INNER JOIN orders or                        INNER JOIN users usr\n",
    "ON usr.customer_id = or.customer_id         ON usr.customer_id = or.customer_id\n",
    "WHERE or.order_date >                       WHERE or.order_date >\n",
    "date_sub(current_date, INTERVAL 30 days)    date_sub(current_date, INTERVAL 30 days)\n",
    "```\n",
    "\n",
    "**13 seconds to run** ------------------------------ **19 seconds to run**\n",
    "\n",
    "Two queries that return exact same results. The only difference is we flip the tables, and it makes 6 seconds speed-up.\n",
    "\n",
    "### Why Temporary Tables Rock\n",
    "\n",
    "- Temporary tables improve readability over (nested) subqueries, i.e.\n",
    "\n",
    "```sql\n",
    "SELECT ...\n",
    "INTO #goodnameforatemptable\n",
    "\n",
    "SELECT ... \n",
    "FROM blah\n",
    "JOIN #goodnameforatemptable\n",
    "\n",
    "instead of \n",
    "SELECT ...\n",
    "FROM \n",
    "JOIN (SELECT ... ) ON\n",
    "```\n",
    "\n",
    "- Keeps your code readable and makes troubleshooting much easier.\n",
    "- Helps you follow the single responsibility principle.\n",
    "- The [Query Optimizer](https://en.wikipedia.org/wiki/Query_optimization) may not be able to properly optimize a query with subqueries and will likely result in longer run times.\n",
    "\n",
    "\n",
    "### Some Simple SQL Optimization things to remember\n",
    "\n",
    "- When doing wildcard searches use `something%` (limit it to backend wildcard search) vs. `%something%` if you can.\n",
    "- Functions on indexed columns in the where clause remove the indexing.\n",
    "\n",
    "```sql\n",
    "WHERE substring (column, 1, 1) = 'F'\n",
    "```\n",
    "\n",
    "vs.\n",
    "\n",
    "```sql\n",
    "WHERE column LIKE 'F%'\n",
    "```\n",
    "\n",
    "- Don't pull in columns you don't need.\n",
    "- Move filters from the `WHERE` statement to the `JOIN` condition if using an `OUTER JOIN`.\n",
    "- Use your indices as much as possible.\n",
    "- If you can, use `UNION ALL` instead of `UNION DISTINCT`.\n",
    "\n",
    "## Data Manipulation & Feature Engineering Before Python\n",
    "\n",
    "### Joining Multiple Data Sources\n",
    "\n",
    "- This is SQL's bread and butter, it's wheelhouse, what it was made to do.\n",
    "- Even if you have multiple raw data files, I advocate for standing up a quick DB and loading the CSVs in there vs. bringing them straight into pandas &mdash; you don't have to reload the file every time you want to look at the data.\n",
    "\n",
    "### Narrowing down your dataset\n",
    "\n",
    "- More data isn't always better &mdash; loading 50 million rows of data straight into memory will be incredibly time consuming, inefficient, and does not necessarily lead to better machine learning scores.\n",
    "- Time frame considerations &mdash; do you see a dramatic improvement in model scores with two years of data vs. one year?\n",
    "\n",
    "### Learning Curves to Help Determine Dataset Sizes\n",
    "\n",
    "Learning Curves help you understand not only your algorithm's [bias vs. variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) but also how many records you need to train a model you're happy with.\n",
    "\n",
    "- **High variance** &mdash; If a learning algorithm is suffering from high variance, getting more training data is likely to help.\n",
    "\n",
    "- **High bias** &mdash; If a learning algorithm is suffering from high bias, getting more training data will not (by itself) help much.\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "- Case statements prevent a size mismatch that can sometimes happen with `pandas.get_dummies()` in train vs test sets.\n",
    "- Lead, Lag, Rank functions are powerful calculations that are oddly written in SQL but can leverage indices and run more optimally in SQL.\n",
    "- A lot of features can be created before you even load the data if you'd like.\n",
    "\n",
    "### Pandas\n",
    "\n",
    "- You have the whole Python toolset at your disposal.\n",
    "- You can do more advanced and intensive data manipulation in code that is easily readable and **testable**.\n",
    "- Easy integration with data visualization libraries, jupyter notebooks, and functions like `data_frame.describe()` make it ideal for [EDA](https://en.wikipedia.org/wiki/Exploratory_data_analysis) (exploratory data analysis).\n",
    "\n",
    "## How to Optimize Reading and Writing with Python\n",
    "\n",
    "### Reading in data from SQL\n",
    "\n",
    "- `pandas.read_sql()` and `pandas.read_gbq()` are notoriously slow.\n",
    "- The best way I have found to do this is to save your final query as a table and export that table to CSV or CSV.GZ and then load the CSV into pandas via `pandas.read_csv()`\n",
    "- What this also allows for is then each time moving forward you only have to load the CSV directly.\n",
    "\n",
    "### Writing data to SQL\n",
    "\n",
    "- Similarly, `data_frame.to_sql()`, `data_frame.to_gbq()`, even Spark's `data_frame.write.jdbc()` are also slow.\n",
    "\n",
    "> Batch writing to the rescue.\n",
    "\n",
    "- SQLAlchemy & Pandas:\n",
    "\n",
    "\n",
    "```python\n",
    "Session = sessionmaker(bind=dest_db_con)\n",
    "sess = Session()\n",
    "sess.bulk_insert_mappings(MentorInformation, df.to_dict(orient=\"records\"))\n",
    "sess.close()\n",
    "```\n",
    "\n",
    "- Spark:\n",
    "\n",
    "```shell\n",
    "jdbcUrl = 'jdbc:mysql://{}:3306/{}?useServerPrepStmts=false&rewriteBatchedStatements=true&user{}&password={}'\n",
    "```\n",
    "\n",
    "**&rewriteBatchedStatements=true**\n",
    "\n",
    "With these steps, I've seen jobs that used to take hours now take minutes.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "- Knowing what tool is right for the job is extremely powerful.\n",
    "\n",
    "Notes from:\n",
    "\n",
    "- https://www.youtube.com/watch?v=H5FNFxHgSj8&list=LLqaZUbmWbk33CkdqqWfC1xw&index=2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}